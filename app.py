# -*- coding: utf-8 -*-
"""
PFC æ•…éšœè¯Šæ–­ & ç”µå®¹çŠ¶æ€ç›‘æµ‹ â€”â€” ç»Ÿä¸€å‰ç«¯
"""

import os, re, json, joblib
import numpy as np
import pandas as pd
import streamlit as st

# å¯è§†åŒ–
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import label_binarize
from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,
                             precision_recall_curve, average_precision_score)

# ========== å…¨å±€ä¸­æ–‡å­—ä½“ ==========
from matplotlib import font_manager
def _use_chinese_font(prefer: str|None=None) -> str|None:
    cands = [prefer] if prefer else []
    cands += ["Microsoft YaHei","SimHei","PingFang SC","Hiragino Sans GB",
              "Heiti SC","WenQuanYi Micro Hei","Noto Sans CJK SC","Source Han Sans CN"]
    owned = {f.name for f in font_manager.fontManager.ttflist}
    chosen = None
    for n in cands:
        if n and n in owned:
            chosen = n; break
    if chosen:
        matplotlib.rcParams["font.family"] = [chosen]
        matplotlib.rcParams["font.sans-serif"] = [chosen]
    matplotlib.rcParams["axes.unicode_minus"] = False
    return chosen
_CN_FONT = _use_chinese_font()

# ========== rerun å…¼å®¹ ==========
def _safe_rerun():
    if hasattr(st, "rerun"): st.rerun()
    elif hasattr(st, "experimental_rerun"): st.experimental_rerun()

# ========== ç™»å½• ==========
def check_login(user: str, pwd: str) -> bool:
    try:
        auth = st.secrets.get("auth", {})
        u = auth.get("username", "admin")
        p = auth.get("password", "pfc@123")
    except Exception:
        u, p = "admin", "pfc@123"
    return (user == u) and (pwd == p)

def login_block() -> bool:
    if st.session_state.get("authed", False):
        return True
    st.title("ğŸ” ç™»å½•ç³»ç»Ÿ")
    with st.form("login"):
        user = st.text_input("ç”¨æˆ·å", "")
        pwd  = st.text_input("å¯†ç ", "", type="password")
        ok   = st.form_submit_button("ç™»å½•")
    if ok:
        if check_login(user, pwd):
            st.session_state["authed"] = True
            st.success("ç™»å½•æˆåŠŸ")
            _safe_rerun()
        else:
            st.error("ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
    return False

# ========== æ•…éšœè¯Šæ–­å¸¸é‡ ==========
FINAL_FEATURES = [
    "åŸå§‹æœ‰æ•ˆå€¼","åŸå§‹å³°å€¼","åŸå§‹è°·å€¼","åŸå§‹å³°å³°å€¼",
    "è°·å€¼/å³°å€¼","å³°å³°å€¼/æœ‰æ•ˆå€¼","å³°å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼","è°·å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼"
]
BASE_FEATURES = ["åŸå§‹æœ‰æ•ˆå€¼","åŸå§‹å³°å€¼","åŸå§‹è°·å€¼","åŸå§‹å³°å³°å€¼"]

DEFAULT_VERSION_FILE = {
    "RIMER":    "model_version.rimer.json",
    "RF":       "model_version.rf.json",
    "SVM":      "model_version.svm.json",
    "LR":       "model_version.lr.json",
    "Ensemble": "model_version.ensemble.json",
}

FILENAME_RULES = [
    {"pattern": r"normal",                "label": "æ­£å¸¸æ¨¡å¼"},
    {"pattern": r"Q1[_-]?mode",           "label": "Q1å¼€è·¯"},
    {"pattern": r"Q1[_-]?open.*Q2[_-]?short", "label": "Q1å¼€è·¯+Q2çŸ­è·¯"},
    {"pattern": r"Q1[_-]?short(?!.*Q3[_-]?open)", "label": "Q1çŸ­è·¯"},
    {"pattern": r"Q1[_-]?short.*Q3[_-]?open",     "label": "Q1çŸ­è·¯+Q3å¼€è·¯"},
    {"pattern": r"Q2[_-]?mode",           "label": "Q2å¼€è·¯"},
    {"pattern": r"Q2[_-]?short",          "label": "Q2çŸ­è·¯"},
    {"pattern": r"Q3[_-]?mode",           "label": "Q3å¼€è·¯"},
    {"pattern": r"Q3[_-]?open.*Q4[_-]?short",     "label": "Q3å¼€è·¯+Q4çŸ­è·¯"},
    {"pattern": r"Q3[_-]?short(?!.*Q4[_-]?open)", "label": "Q3çŸ­è·¯"},
    {"pattern": r"Q4[_-]?mode",           "label": "Q4å¼€è·¯"},
    {"pattern": r"Q4[_-]?short",          "label": "Q4çŸ­è·¯"},
]

ALIASES = {
    "åŸå§‹æœ‰æ•ˆå€¼": ["åŸå§‹æœ‰æ•ˆå€¼","å¤„ç†åæœ‰æ•ˆå€¼","å½’ä¸€åŒ–æœ‰æ•ˆå€¼","æœ‰æ•ˆå€¼"],
    "åŸå§‹å³°å€¼":   ["åŸå§‹å³°å€¼","å¤„ç†å³°å€¼","å½’ä¸€åŒ–å³°å€¼","å³°å€¼"],
    "åŸå§‹è°·å€¼":   ["åŸå§‹è°·å€¼","å¤„ç†è°·å€¼","å½’ä¸€åŒ–è°·å€¼","è°·å€¼"],
    "åŸå§‹å³°å³°å€¼": ["åŸå§‹å³°å³°å€¼","å¤„ç†å³°å³°å€¼","å½’ä¸€åŒ–å³°å³°å€¼","å³°å³°å€¼"],
}

# ========== å·¥å…·å‡½æ•° ==========
def read_csv_fallback(file):
    try:
        return pd.read_csv(file, encoding="gbk")
    except Exception:
        try:
            if hasattr(file, "seek"): file.seek(0)
        except Exception: pass
        return pd.read_csv(file, encoding="utf-8")

def _collapse_duplicates_keep_first(df: pd.DataFrame, name: str) -> pd.DataFrame:
    cols = [c for c in df.columns if c == name]
    if len(cols) > 1:
        df = df.drop(columns=cols[1:])
    return df

def coerce_and_derive_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.rename(columns=lambda x: str(x).strip())
    # åˆ«å â†’ æ ‡å‡†å
    rename_map = {}
    for canon, alts in ALIASES.items():
        if canon in df.columns: continue
        for a in alts:
            if a in df.columns and a != canon:
                rename_map[a] = canon; break
    if rename_map:
        df = df.rename(columns=rename_map)
    # ç¡®ä¿åŸºç¡€åˆ—
    for b in BASE_FEATURES:
        df = _collapse_duplicates_keep_first(df, b)
        if b not in df.columns:
            raise ValueError(f"ç¼ºå°‘åŸºç¡€åˆ—ï¼š{b}")
    # æ´¾ç”Ÿæ¯”å€¼
    if "è°·å€¼/å³°å€¼" not in df.columns:
        df["è°·å€¼/å³°å€¼"] = df["åŸå§‹è°·å€¼"]/(df["åŸå§‹å³°å€¼"]+1e-8)
    if "å³°å³°å€¼/æœ‰æ•ˆå€¼" not in df.columns:
        df["å³°å³°å€¼/æœ‰æ•ˆå€¼"] = df["åŸå§‹å³°å³°å€¼"]/(df["åŸå§‹æœ‰æ•ˆå€¼"]+1e-8)
    if "å³°å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼" not in df.columns:
        df["å³°å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼"] = df["åŸå§‹å³°å€¼"]/(df["åŸå§‹æœ‰æ•ˆå€¼"]+1e-8)
    if "è°·å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼" not in df.columns:
        df["è°·å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼"] = df["åŸå§‹è°·å€¼"]/(df["åŸå§‹æœ‰æ•ˆå€¼"]+1e-8)
    # å»é‡å¤åˆ—
    for n in FINAL_FEATURES:
        df = _collapse_duplicates_keep_first(df, n)
    return df

def infer_label_from_name(name: str, rules: list) -> str | None:
    for r in rules:
        if re.search(r["pattern"], name, flags=re.I):
            return r["label"]
    return None

def build_eval_set(uploaded_files, drop_dup: bool, sigma_mode: str, filename_rules):
    TRAIN_ROWS, TOTAL_TAKE = 75, 100
    TEST_ROWS = TOTAL_TAKE - TRAIN_ROWS
    X_list, y_list, used_labels = [], [], []
    per_class_buckets = {}

    for f in uploaded_files:
        df = read_csv_fallback(f).dropna()
        df = coerce_and_derive_features(df)
        if drop_dup:
            df = df.drop_duplicates(subset=FINAL_FEATURES)

        valid = df[FINAL_FEATURES].dropna().head(TOTAL_TAKE)
        if len(valid) < TEST_ROWS:
            continue
        test_df = valid.iloc[TRAIN_ROWS:TOTAL_TAKE]
        label = infer_label_from_name(f.name, filename_rules)
        if label is None:
            st.warning(f"æ— æ³•ä»æ–‡ä»¶åæ¨æ–­æ ‡ç­¾ï¼Œå·²è·³è¿‡ï¼š{f.name}")
            continue

        X_list.append(test_df.values)
        y_list.extend([label]*len(test_df))
        used_labels.append(label)
        if sigma_mode == "per_class":
            per_class_buckets.setdefault(label, []).append(test_df.values)

    if not X_list:
        return np.empty((0, len(FINAL_FEATURES))), np.array([], dtype=object), []

    X_test = np.vstack(X_list)
    y_test = np.array(y_list, dtype=object)

    if sigma_mode == "global" and len(X_test) > 0:
        z = np.abs((X_test - X_test.mean(axis=0)) / (X_test.std(axis=0) + 1e-8))
        keep = np.all(z < 3, axis=1)
        X_test, y_test = X_test[keep], y_test[keep]
        st.info(f"[å…¨å±€3Ïƒ] è¿‡æ»¤åï¼šæµ‹è¯•æ ·æœ¬ {len(y_test)}")
    elif sigma_mode == "per_class":
        kept_X, kept_y = [], []
        for lab, mats in per_class_buckets.items():
            Xi = np.vstack(mats)
            z = np.abs((Xi - Xi.mean(axis=0)) / (Xi.std(axis=0) + 1e-8))
            keep = np.all(z < 3, axis=1)
            kept_X.append(Xi[keep]); kept_y.extend([lab]*np.sum(keep))
        if kept_X:
            X_test = np.vstack(kept_X); y_test = np.array(kept_y, dtype=object)
            st.info(f"[æŒ‰ç±»3Ïƒ] è¿‡æ»¤åï¼šæµ‹è¯•æ ·æœ¬ {len(y_test)}")

    classes = sorted(list(set(used_labels)))
    return X_test, y_test, classes

def load_version_json(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def load_model(version_path: str):
    info = load_version_json(version_path)
    mpath = info.get("model_path")
    if not os.path.isabs(mpath):
        base = os.path.dirname(os.path.abspath(__file__))
        mpath = os.path.join(base, mpath)
    try:
        if "rimer" in os.path.basename(mpath).lower():
            from rimer_model import OptimizedRIMER  # noqa
    except Exception:
        pass
    model = joblib.load(mpath)
    classes = info.get("class_order", [])
    features = info.get("features", FINAL_FEATURES)
    return model, classes, features, info

def _get_pred_and_scores(model, X: np.ndarray, all_classes: list[str]):
    y_pred = model.predict(X)
    model_classes = getattr(model, "classes_", None)
    y_score = None
    if hasattr(model, "predict_proba"):
        try:
            y_score = model.predict_proba(X)
        except Exception:
            y_score = None
    if y_score is None and hasattr(model, "decision_function"):
        try:
            df = model.decision_function(X)
            if df.ndim == 1:
                df = np.vstack([-df, df]).T
            df = df - df.max(axis=1, keepdims=True)
            exp = np.exp(df); y_score = exp/(exp.sum(axis=1, keepdims=True)+1e-8)
        except Exception:
            y_score = None
    if y_score is None:
        base_classes = list(model_classes) if model_classes is not None else all_classes
        y_score = np.zeros((len(y_pred), len(base_classes)))
        idx = {c:i for i,c in enumerate(base_classes)}
        for i,c in enumerate(y_pred):
            if c in idx: y_score[i, idx[c]] = 1.0
        model_classes = base_classes

    if model_classes is None:
        model_classes = all_classes
    model_classes = list(model_classes)
    aligned = np.zeros((len(y_pred), len(all_classes)))
    pos = {c:i for i,c in enumerate(model_classes)}
    for j,c in enumerate(all_classes):
        if c in pos: aligned[:,j] = y_score[:,pos[c]]
    return y_pred, aligned

def _smooth(arr, win: int = 11):
    n = len(arr)
    if n < 5: return arr
    win = max(5, min(win, n if n%2==1 else n-1))
    k = np.ones(win)/win
    pad = np.r_[arr[0], arr, arr[-1]]
    sm = np.convolve(pad, k, mode="same")[1:-1]
    return sm

def _plot_pr_curves(y_true, y_score, classes):
    y_bin = label_binarize(y_true, classes=classes)
    p_micro, r_micro, _ = precision_recall_curve(y_bin.ravel(), y_score.ravel())
    ap_micro = average_precision_score(y_bin, y_score, average="micro")
    fig, ax = plt.subplots(figsize=(7.2, 4.6), dpi=140)
    ax.plot(_smooth(r_micro), _smooth(p_micro), lw=2.2, label=f"micro-avg AP={ap_micro:.3f}")
    for i,c in enumerate(classes):
        p,r,_ = precision_recall_curve(y_bin[:,i], y_score[:,i])
        ax.plot(_smooth(r), _smooth(p), lw=1.2, alpha=.85, label=str(c))
    ax.set_xlim([0,1]); ax.set_ylim([0,1.05])
    ax.set_xlabel("å¬å›ç‡"); ax.set_ylabel("ç²¾ç¡®ç‡"); ax.set_title("PR æ›²çº¿ï¼ˆå¹³æ»‘ï¼‰")
    ax.legend(ncol=2, fontsize=8); ax.grid(alpha=.3, linestyle="--"); plt.tight_layout()
    return fig

def eval_and_plot(model, X_test: np.ndarray, y_test: np.ndarray, all_classes: list[str]):
    if X_test.size == 0:
        st.warning("æµ‹è¯•é›†ä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°ã€‚"); return
    y_pred, y_score = _get_pred_and_scores(model, X_test, all_classes)
    acc = accuracy_score(y_test, y_pred)
    st.subheader(f"âœ… Accuracyï¼š{acc:.4f}")

    cm = confusion_matrix(y_test, y_pred, labels=all_classes)
    fig, ax = plt.subplots(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
                xticklabels=all_classes, yticklabels=all_classes, ax=ax)
    ax.set_xlabel("é¢„æµ‹ç±»åˆ«"); ax.set_ylabel("å®é™…ç±»åˆ«")
    ax.set_title(f"æ··æ·†çŸ©é˜µï¼ˆAccuracy = {acc:.4f}ï¼‰")
    ax.set_xticklabels(all_classes, rotation=45, ha='right')
    ax.set_yticklabels(all_classes, rotation=0)
    plt.tight_layout(); st.pyplot(fig)

    rep = classification_report(y_test, y_pred, labels=all_classes, output_dict=True, zero_division=0)
    rep_df = pd.DataFrame(rep).T
    st.subheader("åˆ†ç±»æŠ¥å‘Š"); st.dataframe(rep_df.style.format(precision=4), use_container_width=True)

    appeared = sorted(list({c for c in y_test if c in all_classes}))
    if len(appeared) >= 2:
        col_idx = [all_classes.index(c) for c in appeared]
        st.subheader("PR æ›²çº¿ï¼ˆå¹³æ»‘ï¼‰")
        st.pyplot(_plot_pr_curves(y_test, y_score[:, col_idx], appeared))
    else:
        st.info("PR æ›²çº¿ï¼šè¯„ä¼°é›†ä¸­å‡ºç°çš„æœ‰æ•ˆç±»åˆ«ä¸è¶³ 2 ä¸ªï¼Œæš‚ä¸ç»˜åˆ¶ã€‚")

# ========== ç”µå®¹ç›‘æµ‹ ==========
CAP_VERSION_FILE_DEFAULT = "model_version.cap_monitor.json"

def _read_csv_any(fp):
    try:
        return pd.read_csv(fp, encoding="gbk")
    except Exception:
        try:
            if hasattr(fp,"seek"): fp.seek(0)
        except Exception: pass
        return pd.read_csv(fp, encoding="utf-8")

def _cap_parse_labels_from_name(name: str):
    cap_m = re.search(r'C=([0-9\.]+(?:e[-+]?\d+)?)(Î¼F|uF|F)', name, flags=re.I)
    res_m = re.search(r'R=([0-9\.]+(?:e[-+]?\d+)?)(mÎ©|Î©)', name, flags=re.I)
    C_uF = R_ohm = None
    if cap_m:
        v,u = cap_m.groups(); v = float(v)
        C_uF = v if u.lower()!='f' else v*1e6
    if res_m:
        v,u = res_m.groups(); v=float(v)
        R_ohm = v/1e3 if u.lower().startswith('m') else v
    return C_uF, R_ohm

def _cap_estimate_raw_params(t, v, i):
    eps = 1e-12
    dv = v[0]-v[-1]; di = (i[0]-i[-1])+eps
    esr_raw = dv/di
    q = float(np.trapz(i, t))
    c_raw = q/(dv+eps)
    return esr_raw, c_raw

def cap_extract_features_from_csv(file_like):
    df = _read_csv_any(file_like)
    t = df.iloc[:,0].to_numpy(); v = df.iloc[:,1].to_numpy(); i = df.iloc[:,2].to_numpy()
    v_pp = float(np.ptp(v)); i_pp=float(np.ptp(i))
    v_rms = float(np.sqrt(np.mean(v**2))); i_rms = float(np.sqrt(np.mean(i**2)))
    dvdt = np.gradient(v, t); max_dvdt = float(np.max(np.abs(dvdt)))
    esr_raw, c_raw = _cap_estimate_raw_params(t, v, i)
    return [v_pp, i_pp, v_rms, i_rms, max_dvdt, esr_raw, c_raw]

def _store_cap_model_to_state(model, features, info, version_file):
    st.session_state["cap_mdl"] = model
    st.session_state["cap_features"] = features
    st.session_state["cap_info"] = info
    st.session_state["cap_version_file"] = version_file

def _get_cap_model_from_state():
    return (st.session_state.get("cap_mdl"),
            st.session_state.get("cap_features", []),
            st.session_state.get("cap_info", {}),
            st.session_state.get("cap_version_file"))

def cap_monitor_page():
    st.header("ç”µå®¹çŠ¶æ€ç›‘æµ‹")

    st.subheader("â‘  åŠ è½½ç”µå®¹ç›‘æµ‹æ¨¡å‹")
    ver_input = st.text_input("model_version.cap_monitor.json è·¯å¾„ï¼ˆç•™ç©ºç”¨é»˜è®¤ï¼‰",
                              value=CAP_VERSION_FILE_DEFAULT)
    ver_path = ver_input if os.path.isabs(ver_input) else os.path.join(os.path.dirname(os.path.abspath(__file__)), ver_input)

    c1, c2 = st.columns(2)
    with c1:
        if st.button("åŠ è½½/åˆ·æ–° ç”µå®¹æ¨¡å‹", type="primary"):
            if not os.path.exists(ver_path):
                st.error(f"ç‰ˆæœ¬æ–‡ä»¶ä¸å­˜åœ¨ï¼š{ver_path}")
            else:
                try:
                    info = load_version_json(ver_path)
                    mpath = info.get("model_path","")
                    if not os.path.isabs(mpath):
                        mpath = os.path.join(os.path.dirname(os.path.abspath(__file__)), mpath)
                    model = joblib.load(mpath)
                    feats = info.get("features", [])
                    _store_cap_model_to_state(model, feats, info, ver_path)
                    st.success(f"æ¨¡å‹å·²åŠ è½½ï¼š{os.path.basename(ver_path)}")
                except Exception as e:
                    st.exception(e)
    with c2:
        if st.button("æ¸…é™¤ç”µå®¹æ¨¡å‹"):
            for k in ["cap_mdl","cap_features","cap_info","cap_version_file"]:
                st.session_state.pop(k, None)
            st.info("å·²æ¸…é™¤ç”µå®¹æ¨¡å‹")

    st.subheader("â‘¡ ä¸Šä¼ ç”µå®¹æ•°æ® CSVï¼ˆå¯å¤šé€‰ï¼‰")
    files = st.file_uploader("æ¯ä¸ªCSVéœ€åŒ…å«å‰ä¸‰åˆ—ï¼štime(s), V, Iï¼›å…¶ä½™åˆ—å¿½ç•¥ã€‚",
                             type=["csv"], accept_multiple_files=True)

    if st.button("å¼€å§‹ç›‘æµ‹"):
        model, feats, info, vfile = _get_cap_model_from_state()
        if model is None:
            st.error("å°šæœªåŠ è½½ç”µå®¹æ¨¡å‹"); st.stop()
        if not files:
            st.warning("è¯·å…ˆä¸Šä¼  CSV æ–‡ä»¶"); st.stop()

        rows = []
        for f in files:
            try:
                X = np.array([cap_extract_features_from_csv(f)], dtype=float)
                yhat = model.predict(X)[0]  # [C_pred, R_pred]
                name = f.name
                C_true, R_true = _cap_parse_labels_from_name(name)
                row = {"file": name,
                       "C_pred(uF)": float(yhat[0]), "R_pred(ohm)": float(yhat[1]),
                       "C_true(uF)": C_true, "R_true(ohm)": R_true}
                if C_true is not None:
                    row["C_err(%)"] = abs(yhat[0]-C_true)/max(C_true,1e-12)*100
                if R_true is not None:
                    row["R_err(%)"] = abs(yhat[1]-R_true)/max(R_true,1e-12)*100
                rows.append(row)
            except Exception as e:
                rows.append({"file": f.name, "error": str(e)})

        df_out = pd.DataFrame(rows)
        st.dataframe(df_out, use_container_width=True)

        if "C_err(%)" in df_out or "R_err(%)" in df_out:
            st.subheader("â‘¢ è¯¯å·®æ±‡æ€»ï¼ˆä»…å½“æ–‡ä»¶åå«çœŸå€¼æ—¶ï¼‰")
            if "C_err(%)" in df_out and df_out["C_err(%)"].notna().any():
                st.write("C(uF) å¹³å‡ç›¸å¯¹è¯¯å·®ï¼š", f"{df_out['C_err(%)'].dropna().mean():.2f}%")
            if "R_err(%)" in df_out and df_out["R_err(%)"].notna().any():
                st.write("R(Î©) å¹³å‡ç›¸å¯¹è¯¯å·®ï¼š", f"{df_out['R_err(%)'].dropna().mean():.2f}%")

        csv_bytes = df_out.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        st.download_button("â¬‡ï¸ ä¸‹è½½ç›‘æµ‹ç»“æœ CSV", data=csv_bytes,
                           file_name="cap_monitor_results.csv", mime="text/csv")

# ========== æ•…éšœè¯Šæ–­ ==========
def diagnosis_page():
    st.title("PFC æ•…éšœè¯Šæ–­ â€” å¤šç®—æ³•æ‰¹é‡æ¨ç†ä¸è¯„ä¼°")

    # ä¾§è¾¹æ ï¼šç®—æ³•é€‰æ‹© & ç‰ˆæœ¬æ–‡ä»¶
    st.sidebar.header("ç®—æ³• / æ¨¡å‹")
    algo = st.sidebar.selectbox("é€‰æ‹©ç®—æ³• / æ¨¡å‹", list(DEFAULT_VERSION_FILE.keys()), index=0)
    custom_ver = st.sidebar.text_input("æˆ–è‡ªå®šä¹‰ model_version.json è·¯å¾„ï¼ˆç•™ç©ºä½¿ç”¨ä¸Šæ–¹é€‰æ‹©ï¼‰", "")

    # ç±»åˆ«æ˜ å°„è§„åˆ™ï¼ˆå¯è¢« class_mapping.json è¦†ç›–ï¼‰
    filename_rules = FILENAME_RULES
    if os.path.exists("class_mapping.json"):
        try:
            with open("class_mapping.json", "r", encoding="utf-8") as f:
                cm = json.load(f)
            filename_rules = cm.get("filename_rules", filename_rules)
        except Exception:
            pass

    # â€”â€” æ•°æ®é¢„å¤„ç†é€‰é¡¹ï¼šé»˜è®¤æ”¶èµ· â€”â€” #
    with st.expander("æ•°æ®é¢„å¤„ç†é€‰é¡¹", expanded=False):
        drop_dup = st.checkbox("å•æ–‡ä»¶æ ·æœ¬å»é‡ï¼ˆæŒ‰ 8 ä¸ªç‰¹å¾ drop_duplicatesï¼‰", value=False)
        sigma_mode = st.radio("3Ïƒ å¼‚å¸¸å€¼è¿‡æ»¤", ["å…³é—­","å…¨å±€ 3Ïƒ","æŒ‰ç±»åˆ« 3Ïƒ"], index=0)
        sigma_mode = {"å…³é—­":"off","å…¨å±€ 3Ïƒ":"global","æŒ‰ç±»åˆ« 3Ïƒ":"per_class"}[sigma_mode]
        mode = st.radio("è¿è¡Œæ¨¡å¼", ["è¯„ä¼°ï¼ˆéœ€è¦æ ‡ç­¾ï¼‰","ä»…é¢„æµ‹ï¼ˆä¸éœ€è¦æ ‡ç­¾ï¼‰"], index=0)

    # â€”â€” â‘  åŠ è½½æ¨¡å‹ â€”â€” #
    st.subheader("â‘  åŠ è½½æ¨¡å‹")
    version_file = custom_ver.strip() if custom_ver.strip() else DEFAULT_VERSION_FILE[algo]
    if not os.path.isabs(version_file):
        version_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), version_file)

    c1, c2 = st.columns(2)
    with c1:
        if st.button("åŠ è½½/åˆ·æ–°æ¨¡å‹", type="primary"):
            if not os.path.exists(version_file):
                st.error(f"ç‰ˆæœ¬æ–‡ä»¶ä¸å­˜åœ¨ï¼š{version_file}")
            else:
                try:
                    model, classes_in_model, features_in_model, info_json = load_model(version_file)
                    st.session_state["dx_model"] = model
                    st.session_state["dx_classes"] = classes_in_model
                    st.session_state["dx_features"] = features_in_model
                    st.session_state["dx_info"] = info_json
                    st.success(f"æ¨¡å‹å·²åŠ è½½ï¼š {os.path.basename(version_file)}")
                except Exception as e:
                    st.exception(e)
    with c2:
        if st.button("æ¸…é™¤å·²åŠ è½½æ¨¡å‹"):
            for k in ["dx_model","dx_classes","dx_features","dx_info"]:
                st.session_state.pop(k, None)
            st.session_state["manual_expanded"] = False
            st.info("å·²æ¸…é™¤æ¨¡å‹")

    # â€”â€” â‘¡ ä¸Šä¼  CSV â€”â€” #
    st.subheader("â‘¡ ä¸Šä¼  CSVï¼ˆå¯å¤šé€‰ï¼‰")
    files = st.file_uploader("CSV åˆ—éœ€åŒ…å«å››ä¸ªåŸºç¡€é‡ï¼ˆæˆ–å…¶åˆ«åï¼‰ï¼›ç³»ç»Ÿä¼šæ´¾ç”Ÿæ¯”å€¼åˆ—ã€‚",
                             type=["csv"], accept_multiple_files=True)

    # â€”â€” è¯„ä¼° â€”â€” #
    if mode == "è¯„ä¼°ï¼ˆéœ€è¦æ ‡ç­¾ï¼‰":
        if st.button("å¼€å§‹è¯„ä¼°", type="secondary"):
            model = st.session_state.get("dx_model")
            classes_in_model = st.session_state.get("dx_classes", [])
            if model is None:
                try:
                    model, classes_in_model, features_in_model, info_json = load_model(version_file)
                    st.session_state["dx_model"] = model
                    st.session_state["dx_classes"] = classes_in_model
                    st.session_state["dx_features"] = features_in_model
                    st.session_state["dx_info"] = info_json
                    st.info("å·²æ ¹æ®é€‰æ‹©è‡ªåŠ¨åŠ è½½æ¨¡å‹ã€‚")
                except Exception as e:
                    st.error("æœªèƒ½åŠ è½½æ¨¡å‹ï¼Œè¯·å…ˆç‚¹å‡»â€œåŠ è½½æ¨¡å‹â€ã€‚"); st.stop()
            if not files:
                st.warning("è¯·å…ˆä¸Šä¼  CSV æ–‡ä»¶ã€‚"); st.stop()

            X_test, y_test, classes_used = build_eval_set(files, drop_dup, sigma_mode, filename_rules)
            all_classes = classes_in_model if classes_in_model else sorted(list(set(classes_used)))
            st.write(f"æµ‹è¯•æ ·æœ¬æ•°ï¼š{len(y_test)}ï¼›ç±»åˆ«æ•°ï¼š{len(all_classes)}")
            if len(y_test) == 0:
                st.warning("æ²¡æœ‰å¯è¯„ä¼°æ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶åæ˜ å°„æ˜¯å¦æ­£ç¡®ã€‚")
            else:
                eval_and_plot(model, X_test, y_test, all_classes)

    # â€”â€” ä»…é¢„æµ‹ï¼ˆå›ºå®šå25è¡Œï¼‰â€”â€” #
    else:
        if st.button("å¼€å§‹é¢„æµ‹", type="secondary"):
            model = st.session_state.get("dx_model")
            classes_in_model = st.session_state.get("dx_classes", [])
            if model is None:
                try:
                    model, classes_in_model, features_in_model, info_json = load_model(version_file)
                    st.session_state["dx_model"] = model
                    st.session_state["dx_classes"] = classes_in_model
                    st.session_state["dx_features"] = features_in_model
                    st.session_state["dx_info"] = info_json
                    st.info("å·²æ ¹æ®é€‰æ‹©è‡ªåŠ¨åŠ è½½æ¨¡å‹ã€‚")
                except Exception as e:
                    st.error("æœªèƒ½åŠ è½½æ¨¡å‹ï¼Œè¯·å…ˆç‚¹å‡»â€œåŠ è½½æ¨¡å‹â€ã€‚"); st.stop()
            if not files:
                st.warning("è¯·å…ˆä¸Šä¼  CSV æ–‡ä»¶ã€‚"); st.stop()

            TRAIN_ROWS, TOTAL_TAKE = 75, 100
            rows = []
            for f in files:
                df = read_csv_fallback(f).dropna()
                df = coerce_and_derive_features(df)
                if drop_dup: df = df.drop_duplicates(subset=FINAL_FEATURES)
                valid = df[FINAL_FEATURES].dropna().head(TOTAL_TAKE)
                part = valid.iloc[TRAIN_ROWS:TOTAL_TAKE]  # å›ºå®šå25è¡Œ
                if len(part)==0:
                    rows.append({"file": f.name, "error":"æœ‰æ•ˆè¡Œæ•°ä¸è¶³"}); continue
                X = part.values
                if sigma_mode == "global":
                    z = np.abs((X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8))
                    keep = np.all(z < 3, axis=1)
                    X = X[keep]
                y_pred = model.predict(X)
                if hasattr(model,"predict_proba"):
                    prob = model.predict_proba(X)
                    avg = np.mean(prob, axis=0)
                    top_idx = int(np.argmax(avg))
                    base_classes = getattr(model,"classes_",None) or classes_in_model
                    top_label = base_classes[top_idx] if base_classes else str(np.unique(y_pred)[0])
                    conf = float(np.max(avg))
                else:
                    vals, cnts = np.unique(y_pred, return_counts=True)
                    top_label = str(vals[int(np.argmax(cnts))]); conf = np.nan
                rows.append({"file": f.name, "samples": len(X),
                             "majority_pred": top_label, "avg_conf": conf})
            out = pd.DataFrame(rows)
            st.dataframe(out, use_container_width=True)

            csv_bytes = out.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
            st.download_button("â¬‡ï¸ ä¸‹è½½é¢„æµ‹ç»“æœ CSV", data=csv_bytes,
                               file_name="diagnosis_predictions.csv", mime="text/csv")

    # â€”â€” â‘¢ æ‰‹åŠ¨è¾“å…¥å•æ ·æœ¬é¢„æµ‹ï¼ˆäºŒé€‰ä¸€æ˜¾ç¤ºï¼‰ â€”â€” #
    st.subheader("â‘¢ æ‰‹åŠ¨è¾“å…¥å•æ ·æœ¬é¢„æµ‹")
    with st.expander("æ‰‹åŠ¨è¾“å…¥å¹¶é¢„æµ‹", expanded=st.session_state.get("manual_expanded", False)):
        model = st.session_state.get("dx_model")
        if model is None:
            st.info("è¯·å…ˆåœ¨ä¸Šæ–¹â€œâ‘  åŠ è½½æ¨¡å‹â€ä¸­åŠ è½½æ¨¡å‹ã€‚")
        else:
            with st.form("manual_predict_form"):
                input_mode = st.radio(
                    "è¾“å…¥æ–¹å¼",
                    ["åªå¡« 4 ä¸ªåŸºç¡€é‡ï¼ˆè‡ªåŠ¨æ´¾ç”Ÿ 4 ä¸ªæ¯”å€¼ï¼‰", "ç›´æ¥å¡« 8 ä¸ªæœ€ç»ˆç‰¹å¾"],
                    index=0
                )

                if input_mode == "åªå¡« 4 ä¸ªåŸºç¡€é‡ï¼ˆè‡ªåŠ¨æ´¾ç”Ÿ 4 ä¸ªæ¯”å€¼ï¼‰":
                    v_rms = st.number_input("åŸå§‹æœ‰æ•ˆå€¼", value=0.0, format="%.6f")
                    v_pk  = st.number_input("åŸå§‹å³°å€¼",   value=0.0, format="%.6f")
                    v_val = st.number_input("åŸå§‹è°·å€¼",   value=0.0, format="%.6f")
                    v_pp  = st.number_input("åŸå§‹å³°å³°å€¼", value=0.0, format="%.6f")
                    submitted = st.form_submit_button("é¢„æµ‹")

                    if submitted:
                        feats = {
                            "åŸå§‹æœ‰æ•ˆå€¼": v_rms, "åŸå§‹å³°å€¼": v_pk,
                            "åŸå§‹è°·å€¼": v_val, "åŸå§‹å³°å³°å€¼": v_pp,
                            "è°·å€¼/å³°å€¼": v_val/(v_pk+1e-8),
                            "å³°å³°å€¼/æœ‰æ•ˆå€¼": v_pp/(v_rms+1e-8),
                            "å³°å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼": v_pk/(v_rms+1e-8),
                            "è°·å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼": v_val/(v_rms+1e-8),
                        }
                        X = np.array([[feats[n] for n in FINAL_FEATURES]], dtype=float)
                        y_pred = model.predict(X)[0]
                        st.success(f"é¢„æµ‹ç±»åˆ«ï¼š{y_pred}")

                else:  # ç›´æ¥å¡« 8 ä¸ªæœ€ç»ˆç‰¹å¾
                    feat_vals = {}
                    for n in FINAL_FEATURES:
                        feat_vals[n] = st.number_input(n, value=0.0, format="%.6f", key=f"mf_{n}")
                    submitted = st.form_submit_button("é¢„æµ‹")

                    if submitted:
                        X = np.array([[feat_vals[n] for n in FINAL_FEATURES]], dtype=float)
                        y_pred = model.predict(X)[0]
                        st.success(f"é¢„æµ‹ç±»åˆ«ï¼š{y_pred}")

# ========== ä¸»ç¨‹åº ==========
def main():
    if not login_block():
        return

    st.sidebar.header("åŠŸèƒ½æ¨¡å—")
    module = st.sidebar.radio("é€‰æ‹©æ¨¡å—", ["å¼€å…³ç®¡æ•…éšœè¯Šæ–­","ç”µå®¹çŠ¶æ€ç›‘æµ‹"], index=0)

    if module == "ç”µå®¹çŠ¶æ€ç›‘æµ‹":
        cap_monitor_page()
    else:
        diagnosis_page()

    st.caption("æç¤ºï¼šæ¨¡å‹ç”± `train_export_ml_models_improved.py` ä¸ `train_cap_monitor.py` å¯¼å‡ºï¼Œ"
               "ç‰ˆæœ¬æ–‡ä»¶ï¼ˆmodel_version.*.jsonï¼‰è®°å½•äº†æ¨¡å‹è·¯å¾„ã€ç‰¹å¾é¡ºåºä¸ç±»åˆ«/ç›®æ ‡åã€‚")

if __name__ == "__main__":
    main()
