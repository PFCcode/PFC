# -*- coding: utf-8 -*-
"""
PFC æ•…éšœè¯Šæ–­ â€” å¤šç®—æ³•æ‰¹é‡æ¨ç†ä¸è¯„ä¼°/é¢„æµ‹ï¼ˆå¸¦æ‰‹åŠ¨å•æ ·æœ¬é¢„æµ‹ï¼‰
- ç™»å½•ï¼ˆæ”¯æŒ .streamlit/secrets.tomlï¼›é»˜è®¤ admin/pfc@123ï¼‰
- é€‰æ‹©å¹¶åŠ è½½æ¨¡å‹ï¼ˆRIMER/RF/SVM/LR/Ensembleï¼‰
- è¯„ä¼°ï¼ˆå25è¡Œï¼‰/ ä»…é¢„æµ‹ï¼ˆå¯é€‰å25/å‰75/å‰100ï¼‰
- é¢„å¤„ç†ï¼šæŒ‰8ç‰¹å¾å»é‡ï¼›3Ïƒè¿‡æ»¤ï¼ˆå…³é—­/å…¨å±€/æŒ‰ç±»åˆ«ï¼‰
- æ‰‹åŠ¨è¾“å…¥å•æ ·æœ¬ï¼ˆ4åŸºç¡€é‡æˆ–8ç‰¹å¾ï¼‰â†’ é¢„æµ‹ç±»åˆ«+æ¦‚ç‡
- â˜… ä¿®å¤ï¼šæ¨¡å‹ä¸é…ç½®æŒä¹…åŒ–åˆ° st.session_stateï¼Œé¿å…è¾“å…¥æ¡†è§¦å‘é‡è·‘åâ€œæ¨¡å‹ä¸¢å¤±â€
"""
import os
import re
import io
import json
import joblib
import numpy as np
import pandas as pd
import streamlit as st

# ---------- é¡µé¢é…ç½® ----------
st.set_page_config(page_title="PFC æ•…éšœè¯Šæ–­å¹³å°", page_icon="ğŸ› ï¸", layout="wide")

# ---------- ç™»å½•ä¸æ ‡é¢˜ ----------
USERNAME_DEFAULT = "admin"
PASSWORD_DEFAULT = "pfc@123"
HEADER_TITLE = "PFC æ•…éšœè¯Šæ–­å¹³å° â€” å¤šç®—æ³•é€‰æ‹©æ¨ç†ä¸è¯„ä¼°/é¢„æµ‹"

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import label_binarize
from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,
                             precision_recall_curve, average_precision_score)

plt.rcParams["axes.unicode_minus"] = False
from matplotlib import font_manager
def _use_chinese_font(prefer: str | None = None) -> str | None:
    cands = [prefer] if prefer else []
    cands += ["Microsoft YaHei","SimHei","PingFang SC","Hiragino Sans GB","Heiti SC",
              "STHeiti","Songti SC","KaiTi","FangSong","WenQuanYi Micro Hei",
              "Noto Sans CJK SC","Source Han Sans CN","Arial Unicode MS"]
    owned = {f.name for f in font_manager.fontManager.ttflist}
    ch = next((n for n in cands if n in owned), None)
    if ch:
        matplotlib.rcParams["font.family"] = [ch]
        matplotlib.rcParams["font.sans-serif"] = [ch]
    matplotlib.rcParams["axes.unicode_minus"] = False
    return ch
_ = _use_chinese_font()

# ---------- ä¸è®­ç»ƒè„šæœ¬ä¸€è‡´çš„å¸¸é‡ ----------
FINAL_FEATURES = [
    "åŸå§‹æœ‰æ•ˆå€¼","åŸå§‹å³°å€¼","åŸå§‹è°·å€¼","åŸå§‹å³°å³°å€¼",
    "è°·å€¼/å³°å€¼","å³°å³°å€¼/æœ‰æ•ˆå€¼","å³°å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼","è°·å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼"
]
BASE_FEATURES = ["åŸå§‹æœ‰æ•ˆå€¼","åŸå§‹å³°å€¼","åŸå§‹è°·å€¼","åŸå§‹å³°å³°å€¼"]
TRAIN_ROWS, TOTAL_TAKE = 75, 100
TEST_ROWS = TOTAL_TAKE - TRAIN_ROWS

DEFAULT_VERSION_FILE = {
    "RIMER":    "model_version.rimer.json",
    "RF":       "model_version.rf.json",
    "SVM":      "model_version.svm.json",
    "LR":       "model_version.lr.json",
    "Ensemble": "model_version.ensemble.json",
}
FILENAME_RULES = [
    {"pattern": r"normal", "label": "æ­£å¸¸æ¨¡å¼"},
    {"pattern": r"Q1[_-]?mode", "label": "Q1å¼€è·¯"},
    {"pattern": r"Q1[_-]?open.*Q2[_-]?short", "label": "Q1å¼€è·¯+Q2çŸ­è·¯"},
    {"pattern": r"Q1[_-]?short(?!.*Q3[_-]?open)", "label": "Q1çŸ­è·¯"},
    {"pattern": r"Q1[_-]?short.*Q3[_-]?open", "label": "Q1çŸ­è·¯+Q3å¼€è·¯"},
    {"pattern": r"Q2[_-]?mode", "label": "Q2å¼€è·¯"},
    {"pattern": r"Q2[_-]?short", "label": "Q2çŸ­è·¯"},
    {"pattern": r"Q3[_-]?mode", "label": "Q3å¼€è·¯"},
    {"pattern": r"Q3[_-]?open.*Q4[_-]?short", "label": "Q3å¼€è·¯+Q4çŸ­è·¯"},
    {"pattern": r"Q3[_-]?short(?!.*Q4[_-]?open)", "label": "Q3çŸ­è·¯"},
    {"pattern": r"Q4[_-]?mode", "label": "Q4å¼€è·¯"},
    {"pattern": r"Q4[_-]?short", "label": "Q4çŸ­è·¯"},
]
ALIASES = {
    "åŸå§‹æœ‰æ•ˆå€¼": ["åŸå§‹æœ‰æ•ˆå€¼","å¤„ç†åæœ‰æ•ˆå€¼","å½’ä¸€åŒ–æœ‰æ•ˆå€¼","æœ‰æ•ˆå€¼"],
    "åŸå§‹å³°å€¼":   ["åŸå§‹å³°å€¼","å¤„ç†å³°å€¼","å½’ä¸€åŒ–å³°å€¼","å³°å€¼"],
    "åŸå§‹è°·å€¼":   ["åŸå§‹è°·å€¼","å¤„ç†è°·å€¼","å½’ä¸€åŒ–è°·å€¼","è°·å€¼"],
    "åŸå§‹å³°å³°å€¼": ["åŸå§‹å³°å³°å€¼","å¤„ç†å³°å³°å€¼","å½’ä¸€åŒ–å³°å³°å€¼","å³°å³°å€¼"],
}

# ---------- ç™»å½• ----------
def _safe_rerun():
    if hasattr(st, "rerun"): st.rerun()
    else: st.experimental_rerun()

def check_login(user: str, pwd: str) -> bool:
    try:
        auth = st.secrets.get("auth", {})
        username = auth.get("username", USERNAME_DEFAULT)
        password = auth.get("password", PASSWORD_DEFAULT)
    except Exception:
        username, password = USERNAME_DEFAULT, PASSWORD_DEFAULT
    return (user == username) and (pwd == password)

def _render_header_bar():
    col1, col2 = st.columns([8,1])
    with col1:
        st.markdown(f"<h2 style='text-align:center;margin:0'>{HEADER_TITLE}</h2>", unsafe_allow_html=True)
    with col2:
        if st.button("é€€å‡ºç™»å½•", use_container_width=True):
            st.session_state.clear(); _safe_rerun()
    st.divider()

def login_block() -> bool:
    if st.session_state.get("authed", False):
        _render_header_bar(); return True
    st.title("ğŸ” ç™»å½• PFC æ•…éšœè¯Šæ–­å¹³å°")
    with st.form("login"):
        user = st.text_input("ç”¨æˆ·å", value="")
        pwd  = st.text_input("å¯†ç ", value="", type="password")
        ok   = st.form_submit_button("ç™»å½•")
    if ok:
        if check_login(user, pwd):
            st.session_state["authed"] = True
            st.success("ç™»å½•æˆåŠŸ"); _safe_rerun()
        else:
            st.error("ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
    st.stop()

# ---------- é€šç”¨æ•°æ®å¤„ç† ----------
def read_csv_fallback(file) -> pd.DataFrame:
    try:
        return pd.read_csv(file, encoding="gbk")
    except Exception:
        file.seek(0); return pd.read_csv(file, encoding="utf-8")

def _collapse_duplicates_keep_first(df: pd.DataFrame, name: str) -> pd.DataFrame:
    cols = [c for c in df.columns if c == name]
    if len(cols) > 1: df = df.drop(columns=cols[1:])
    return df

def coerce_and_derive_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.rename(columns=lambda x: str(x).strip())
    # åˆ«å -> æ ‡å‡†å
    rename_map = {}
    for canon, alts in ALIASES.items():
        if canon in df.columns: continue
        for a in alts:
            if a in df.columns and a != canon:
                rename_map[a] = canon; break
    if rename_map: df = df.rename(columns=rename_map)
    for b in BASE_FEATURES:
        df = _collapse_duplicates_keep_first(df, b)
        if b not in df.columns: raise ValueError(f"ç¼ºå°‘åŸºç¡€åˆ—ï¼š{b}")
    # æ´¾ç”Ÿå››ä¸ªæ¯”å€¼
    if "è°·å€¼/å³°å€¼" not in df.columns:
        df["è°·å€¼/å³°å€¼"] = df["åŸå§‹è°·å€¼"] / (df["åŸå§‹å³°å€¼"] + 1e-8)
    if "å³°å³°å€¼/æœ‰æ•ˆå€¼" not in df.columns:
        df["å³°å³°å€¼/æœ‰æ•ˆå€¼"] = df["åŸå§‹å³°å³°å€¼"] / (df["åŸå§‹æœ‰æ•ˆå€¼"] + 1e-8)
    if "å³°å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼" not in df.columns:
        df["å³°å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼"] = df["åŸå§‹å³°å€¼"] / (df["åŸå§‹æœ‰æ•ˆå€¼"] + 1e-8)
    if "è°·å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼" not in df.columns:
        df["è°·å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼"] = df["åŸå§‹è°·å€¼"] / (df["åŸå§‹æœ‰æ•ˆå€¼"] + 1e-8)
    for n in FINAL_FEATURES:
        df = _collapse_duplicates_keep_first(df, n)
    return df

def infer_label_from_name(name: str, rules: list) -> str | None:
    for r in rules:
        if re.search(r["pattern"], name, flags=re.I): return r["label"]
    return None

# ---------- æ„å»ºè¯„ä¼°/é¢„æµ‹é›† ----------
def build_eval_set(uploaded_files, drop_dup: bool, sigma_mode: str, filename_rules):
    X_list, y_list, used_labels = [], [], []
    for f in uploaded_files:
        name = f.name
        df = read_csv_fallback(f).dropna()
        df = coerce_and_derive_features(df)
        if drop_dup:
            n0 = len(df)
            df = df.drop_duplicates(subset=FINAL_FEATURES, keep="first")
            n1 = len(df)
            if n1 < n0: st.info(f"[{name}] å»é‡ï¼š{n0} â†’ {n1}ï¼ˆç§»é™¤ {n0-n1} æ¡ï¼‰")
        valid = df[FINAL_FEATURES].dropna().head(TOTAL_TAKE)
        if len(valid) < TEST_ROWS:
            st.warning(f"[{name}] æœ‰æ•ˆæ ·æœ¬ä¸è¶³ {TEST_ROWS} æ¡ï¼Œå·²è·³è¿‡ã€‚"); continue
        test_df = valid.iloc[TRAIN_ROWS:TOTAL_TAKE]
        label = infer_label_from_name(name, filename_rules)
        if label is None:
            st.warning(f"æ— æ³•ä»æ–‡ä»¶åæ¨æ–­æ ‡ç­¾ï¼Œå·²è·³è¿‡ï¼š{name}"); continue
        X_list.append(test_df.values)
        y_list.extend([label]*len(test_df))
        used_labels.append(label)
    if not X_list:
        return np.empty((0,len(FINAL_FEATURES))), np.array([],dtype=object), []
    X_test = np.vstack(X_list); y_test = np.array(y_list, dtype=object)

    if sigma_mode != "none" and len(X_test) > 0:
        if sigma_mode == "global":
            n0 = len(X_test)
            z = np.abs((X_test - X_test.mean(axis=0))/(X_test.std(axis=0)+1e-8))
            keep = np.all(z < 3, axis=1)
            X_test, y_test = X_test[keep], y_test[keep]
            st.info(f"å…¨å±€ 3Ïƒ è¿‡æ»¤ï¼š{n0} â†’ {len(X_test)}ï¼ˆç§»é™¤ {n0-len(X_test)} æ¡ï¼‰")
        elif sigma_mode == "per_class":
            n0 = len(X_test)
            keep_idx = np.ones(len(y_test), dtype=bool)
            for c in np.unique(y_test):
                mask = (y_test == c); Xc = X_test[mask]
                if len(Xc) == 0: continue
                z = np.abs((Xc - Xc.mean(axis=0))/(Xc.std(axis=0)+1e-8))
                keep_idx[mask] = np.all(z < 3, axis=1)
            X_test, y_test = X_test[keep_idx], y_test[keep_idx]
            st.info(f"æŒ‰ç±»åˆ« 3Ïƒ è¿‡æ»¤ï¼š{n0} â†’ {len(X_test)}ï¼ˆç§»é™¤ {n0-len(X_test)} æ¡ï¼‰")

    classes = sorted(list(set(used_labels)))
    return X_test, y_test, classes

def build_predict_subset(df: pd.DataFrame, range_key: str) -> pd.DataFrame:
    valid = df[FINAL_FEATURES].dropna().head(TOTAL_TAKE)
    if range_key == "rear25": return valid.iloc[TRAIN_ROWS:TOTAL_TAKE]
    if range_key == "front75": return valid.iloc[:TRAIN_ROWS]
    return valid.iloc[:TOTAL_TAKE]

# ---------- æ¨¡å‹åŠ è½½ & ä¼šè¯æŒä¹…åŒ– ----------
def load_version_json(path: str):
    with open(path, "r", encoding="utf-8") as f: return json.load(f)

def _really_load_model(version_path: str):
    info = load_version_json(version_path)
    mdl_rel = info.get("model_path")
    model_path = mdl_rel if os.path.isabs(mdl_rel) else os.path.join(os.path.dirname(os.path.abspath(__file__)), mdl_rel)
    try:
        if "rimer" in os.path.basename(model_path).lower():
            from rimer_model import OptimizedRIMER  # noqa: F401
    except Exception as e:
        st.error("åŠ è½½ RIMER å¤±è´¥ï¼šè¯·ç¡®è®¤ rimer_model.py ä¸ app.py åŒç›®å½•ä¸”å¯å¯¼å…¥ï¼›å¹¶å·²å®‰è£… tqdm"); raise e
    model = joblib.load(model_path)
    classes = info.get("class_order", [])
    features = info.get("features", FINAL_FEATURES)
    return model, classes, features, info

def _store_model_to_state(model, classes, features, info, version_file):
    st.session_state["mdl_obj"] = model
    st.session_state["mdl_classes"] = classes
    st.session_state["mdl_features"] = features
    st.session_state["mdl_info"] = info
    st.session_state["mdl_version_file"] = version_file

def _get_model_from_state():
    return (st.session_state.get("mdl_obj"),
            st.session_state.get("mdl_classes", []),
            st.session_state.get("mdl_features", FINAL_FEATURES),
            st.session_state.get("mdl_info", {}),
            st.session_state.get("mdl_version_file"))

# ---------- è¯„åˆ†/å¯è§†åŒ– ----------
def _smooth(arr, win: int = 11):
    n = len(arr)
    if n < 5: return arr
    win = max(5, min(win, n if n % 2 == 1 else n - 1))
    k = np.ones(win)/win
    pad = np.r_[arr[0], arr, arr[-1]]
    return np.convolve(pad, k, mode="same")[1:-1]

def _get_pred_and_scores(model, X: np.ndarray, all_classes: list[str]):
    y_pred = model.predict(X)
    model_classes = getattr(model, "classes_", None)
    y_score = None
    if hasattr(model, "predict_proba"):
        try: y_score = model.predict_proba(X)
        except Exception: y_score = None
    if y_score is None and hasattr(model, "decision_function"):
        try:
            df = model.decision_function(X)
            if df.ndim == 1: df = np.vstack([-df, df]).T
            df = df - df.max(axis=1, keepdims=True)
            exp = np.exp(df); y_score = exp/(exp.sum(axis=1, keepdims=True)+1e-8)
        except Exception: y_score = None
    if y_score is None:
        base_classes = list(model_classes) if model_classes is not None else all_classes
        y_score = np.zeros((len(y_pred), len(base_classes)))
        idx = {c:i for i,c in enumerate(base_classes)}
        for i,c in enumerate(y_pred):
            if c in idx: y_score[i, idx[c]] = 1.0
        model_classes = base_classes
    if model_classes is None: model_classes = all_classes
    model_classes = list(model_classes)
    aligned = np.zeros((len(y_pred), len(all_classes)))
    pos = {c:i for i,c in enumerate(model_classes)}
    for j,c in enumerate(all_classes):
        if c in pos: aligned[:, j] = y_score[:, pos[c]]
    return y_pred, aligned

def _plot_pr_curves(y_true, y_score, classes):
    y_bin = label_binarize(y_true, classes=classes)
    p_micro, r_micro, _ = precision_recall_curve(y_bin.ravel(), y_score.ravel())
    ap_micro = average_precision_score(y_bin, y_score, average="micro")
    fig, ax = plt.subplots(figsize=(7.2,4.6), dpi=140)
    ax.plot(_smooth(r_micro), _smooth(p_micro), lw=2.2, label=f"micro-avg AP={ap_micro:.3f}")
    for i,c in enumerate(classes):
        p, r, _ = precision_recall_curve(y_bin[:, i], y_score[:, i])
        ax.plot(_smooth(r), _smooth(p), lw=1.2, alpha=0.85, label=str(c))
    ax.set_xlim([0,1]); ax.set_ylim([0,1.05])
    ax.set_xlabel("å¬å›ç‡"); ax.set_ylabel("ç²¾ç¡®ç‡")
    ax.set_title("PR æ›²çº¿ï¼ˆå¹³æ»‘ï¼‰"); ax.legend(ncol=2, fontsize=8); ax.grid(alpha=0.3, linestyle="--")
    plt.tight_layout(); return fig

def eval_and_plot(model, X_test, y_test, all_classes):
    if X_test.size == 0:
        st.warning("æµ‹è¯•é›†ä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°ã€‚"); return
    y_pred, y_score = _get_pred_and_scores(model, X_test, all_classes)
    acc = accuracy_score(y_test, y_pred)
    st.subheader(f"âœ… Accuracyï¼š{acc:.4f}")

    cm = confusion_matrix(y_test, y_pred, labels=all_classes)
    fig, ax = plt.subplots(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
                xticklabels=all_classes, yticklabels=all_classes, ax=ax)
    ax.set_xlabel("é¢„æµ‹ç±»åˆ«"); ax.set_ylabel("å®é™…ç±»åˆ«")
    ax.set_title(f"æ··æ·†çŸ©é˜µï¼ˆAccuracy = {acc:.4f}ï¼‰")
    ax.set_xticklabels(all_classes, rotation=45, ha='right'); ax.set_yticklabels(all_classes, rotation=0)
    plt.tight_layout(); st.pyplot(fig)

    rep = classification_report(y_test, y_pred, labels=all_classes, output_dict=True, zero_division=0)
    rep_df = pd.DataFrame(rep).T
    st.subheader("åˆ†ç±»æŠ¥å‘Š"); st.dataframe(rep_df.style.format(precision=4), use_container_width=True)

    appeared = sorted(list({c for c in y_test if c in all_classes}))
    if len(appeared) >= 2:
        col_idx = [all_classes.index(c) for c in appeared]
        y_score_sub = y_score[:, col_idx]
        st.subheader("PR æ›²çº¿ï¼ˆå¹³æ»‘ï¼‰"); st.pyplot(_plot_pr_curves(y_test, y_score_sub, appeared))
    else:
        st.info("PR æ›²çº¿ï¼šè¯„ä¼°é›†ä¸­å‡ºç°çš„æœ‰æ•ˆç±»åˆ«ä¸è¶³ 2 ä¸ªï¼Œæš‚ä¸ç»˜åˆ¶ã€‚")

# ---------- ä»…é¢„æµ‹æ¨¡å¼å±•ç¤º ----------
def predict_and_show(model, files, drop_dup, sigma_mode_for_predict, range_key, classes_in_model):
    if not files:
        st.warning("è¯·å…ˆä¸Šä¼  CSV æ–‡ä»¶ã€‚"); return
    all_classes = classes_in_model if classes_in_model else list(getattr(model, "classes_", []))

    for f in files:
        name = f.name
        df = read_csv_fallback(f).dropna()
        df = coerce_and_derive_features(df)
        if drop_dup:
            n0 = len(df)
            df = df.drop_duplicates(subset=FINAL_FEATURES, keep="first")
            n1 = len(df)
            if n1 < n0: st.info(f"[{name}] å»é‡ï¼š{n0} â†’ {n1}ï¼ˆç§»é™¤ {n0-n1} æ¡ï¼‰")
        subset = build_predict_subset(df, range_key)
        if len(subset) == 0:
            st.warning(f"[{name}] æ— å¯ç”¨æ ·æœ¬ï¼Œå·²è·³è¿‡ã€‚"); continue

        X = subset.values
        if sigma_mode_for_predict == "global":
            n0 = len(X)
            z = np.abs((X - X.mean(axis=0))/(X.std(axis=0)+1e-8))
            keep = np.all(z < 3, axis=1)
            X = X[keep]; st.info(f"[{name}] å…¨å±€ 3Ïƒ è¿‡æ»¤ï¼š{n0} â†’ {len(X)}ï¼ˆç§»é™¤ {n0-len(X)} æ¡ï¼‰")

        y_pred, y_score = _get_pred_and_scores(model, X, all_classes)
        max_score = y_score.max(axis=1) if y_score is not None and y_score.size else np.full(len(y_pred), np.nan)

        st.subheader(f"ğŸ“„ æ–‡ä»¶ï¼š{name}ï¼ˆæ ·æœ¬ {len(X)}ï¼‰")
        counts = pd.Series(y_pred).value_counts().sort_values(ascending=False)
        maj_label = counts.index[0] if len(counts) else "N/A"
        mean_conf = float(np.nanmean(max_score)) if len(max_score) else float("nan")
        c1, c2 = st.columns(2)
        with c1: st.metric("å¤šæ•°ç±»ï¼ˆMajorityï¼‰", str(maj_label))
        with c2: st.metric("å¹³å‡æœ€å¤§ç½®ä¿¡åº¦", f"{mean_conf:.3f}" if not np.isnan(mean_conf) else "N/A")

        fig, ax = plt.subplots(figsize=(6.5,3.6), dpi=140)
        counts.plot(kind="bar", ax=ax, color="#4CAF50")
        ax.set_ylabel("è®¡æ•°"); ax.set_title("é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ")
        plt.xticks(rotation=45, ha="right"); plt.tight_layout()
        st.pyplot(fig)

        out_df = pd.DataFrame({
            "file": name, "row_index": subset.index.values,
            "pred": y_pred, "max_confidence": max_score
        })
        st.dataframe(out_df.head(1000), use_container_width=True)
        buf = io.StringIO(); out_df.to_csv(buf, index=False, encoding="utf-8-sig")
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è½½é¢„æµ‹ç»“æœï¼ˆ{name}.csvï¼‰",
            data=buf.getvalue().encode("utf-8-sig"),
            file_name=f"pred_{os.path.splitext(name)[0]}.csv",
            mime="text/csv"
        )

# ---------- æ‰‹åŠ¨è¾“å…¥å•æ ·æœ¬é¢„æµ‹ ----------
def _build_single_row_from_inputs(mode_key: str, feature_order: list[str]) -> pd.DataFrame:
    if mode_key == "base4":
        col1, col2 = st.columns(2)
        with col1:
            v_rms = st.number_input("åŸå§‹æœ‰æ•ˆå€¼", value=0.0, format="%.6f")
            v_min = st.number_input("åŸå§‹è°·å€¼", value=0.0, format="%.6f")
        with col2:
            v_max = st.number_input("åŸå§‹å³°å€¼", value=0.0, format="%.6f")
            v_pp  = st.number_input("åŸå§‹å³°å³°å€¼", value=0.0, format="%.6f")
        df = pd.DataFrame([{
            "åŸå§‹æœ‰æ•ˆå€¼": v_rms, "åŸå§‹å³°å€¼": v_max, "åŸå§‹è°·å€¼": v_min, "åŸå§‹å³°å³°å€¼": v_pp
        }])
        df["è°·å€¼/å³°å€¼"] = df["åŸå§‹è°·å€¼"] / (df["åŸå§‹å³°å€¼"] + 1e-8)
        df["å³°å³°å€¼/æœ‰æ•ˆå€¼"] = df["åŸå§‹å³°å³°å€¼"] / (df["åŸå§‹æœ‰æ•ˆå€¼"] + 1e-8)
        df["å³°å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼"] = df["åŸå§‹å³°å€¼"] / (df["åŸå§‹æœ‰æ•ˆå€¼"] + 1e-8)
        df["è°·å€¼ä¸æœ‰æ•ˆå€¼çš„æ¯”å€¼"] = df["åŸå§‹è°·å€¼"] / (df["åŸå§‹æœ‰æ•ˆå€¼"] + 1e-8)
    else:
        vals = {}
        for name in FINAL_FEATURES:
            vals[name] = st.number_input(name, value=0.0, format="%.6f", key=f"in8_{name}")
        df = pd.DataFrame([vals])
    return df.reindex(columns=feature_order)

def single_predict_ui():
    st.subheader("â‘¢ æ‰‹åŠ¨è¾“å…¥å•æ ·æœ¬é¢„æµ‹")
    # ä»ä¼šè¯å–æ¨¡å‹
    model, mdl_classes, mdl_features, _, _ = _get_model_from_state()
    if model is None:
        st.info("è¯·å…ˆåœ¨ä¸Šæ–¹â€œâ‘  åŠ è½½æ¨¡å‹â€å¤„åŠ è½½æ¨¡å‹ã€‚"); return
    all_classes = mdl_classes if mdl_classes else list(getattr(model, "classes_", []))

    with st.expander("æ‰‹åŠ¨è¾“å…¥å¹¶é¢„æµ‹", expanded=False):
        mode_key = st.radio(
            "è¾“å…¥æ–¹å¼",
            options=["base4","final8"], index=0,
            format_func=lambda x: "åªå¡« 4 ä¸ªåŸºç¡€é‡ï¼ˆè‡ªåŠ¨æ´¾ç”Ÿ 4 ä¸ªæ¯”å€¼ï¼‰" if x=="base4" else "ç›´æ¥å¡« 8 ä¸ªæœ€ç»ˆç‰¹å¾"
        )
        df_one = _build_single_row_from_inputs(mode_key, mdl_features)

        if st.button("å•æ ·æœ¬é¢„æµ‹", type="primary"):
            try:
                X = df_one.values.astype(float)
                y_pred, y_score = _get_pred_and_scores(model, X, all_classes)
                pred = y_pred[0]
                st.success(f"é¢„æµ‹ç»“æœï¼š**{pred}**")
                if all_classes and y_score is not None and y_score.size:
                    prob = pd.Series(y_score[0], index=all_classes).sort_values(ascending=False)
                    st.dataframe(prob.to_frame("æ¦‚ç‡").style.format("{:.3f}"), use_container_width=True)
                    fig, ax = plt.subplots(figsize=(6.2,3.6), dpi=140)
                    prob.plot(kind="bar", ax=ax, color="#4CAF50")
                    ax.set_ylabel("æ¦‚ç‡"); ax.set_title("å„ç±»åˆ«æ¦‚ç‡")
                    plt.xticks(rotation=45, ha="right"); plt.tight_layout()
                    st.pyplot(fig)
            except Exception as e:
                st.error(f"é¢„æµ‹å¤±è´¥ï¼š{e}")

# ---------- ä¸»ä½“ ----------
def main():
    if not login_block(): return

    # ä¾§è¾¹æ ï¼šç®—æ³• / ç‰ˆæœ¬æ–‡ä»¶
    st.sidebar.header("ç®—æ³• / æ¨¡å‹")
    algo = st.sidebar.selectbox("é€‰æ‹©ç®—æ³• / æ¨¡å‹", list(DEFAULT_VERSION_FILE.keys()), index=0)
    custom_ver = st.sidebar.text_input("æˆ–è‡ªå®šä¹‰ model_version.json è·¯å¾„ï¼ˆç•™ç©ºåˆ™ä½¿ç”¨ä¸Šæ–¹é€‰æ‹©ï¼‰", "")

    # è¯»å– class_mapping.jsonï¼ˆå¯è¦†ç›–é»˜è®¤æ–‡ä»¶å â†’ æ ‡ç­¾è§„åˆ™ï¼‰
    filename_rules = FILENAME_RULES
    if os.path.exists("class_mapping.json"):
        try:
            with open("class_mapping.json","r",encoding="utf-8") as f:
                cm = json.load(f)
            filename_rules = cm.get("filename_rules", filename_rules)
        except Exception:
            pass

    # é¢„å¤„ç†ä¸è¿è¡Œæ¨¡å¼
    with st.expander("æ•°æ®é¢„å¤„ç†é€‰é¡¹", expanded=False):
        drop_dup = st.checkbox("å•æ–‡ä»¶æ ·æœ¬å»é‡ï¼ˆæŒ‰ 8 ä¸ªç‰¹å¾ drop_duplicatesï¼‰", value=False)
        sigma_mode = st.radio(
            "3Ïƒ å¼‚å¸¸å€¼è¿‡æ»¤",
            options=["none","global","per_class"], index=0,
            format_func=lambda x: {"none":"å…³é—­","global":"å…¨å±€ 3Ïƒ","per_class":"æŒ‰ç±»åˆ« 3Ïƒ"}[x]
        )
        mode = st.radio(
            "è¿è¡Œæ¨¡å¼", options=["eval","predict"], index=0,
            format_func=lambda x: "è¯„ä¼°ï¼ˆéœ€è¦æ ‡ç­¾ï¼‰" if x=="eval" else "ä»…é¢„æµ‹ï¼ˆä¸éœ€è¦æ ‡ç­¾ï¼‰"
        )
        if mode == "predict":
            range_key = st.selectbox(
                "ä»…é¢„æµ‹æ¨¡å¼ï¼šå–æ ·èŒƒå›´",
                options=[("rear25","å25è¡Œï¼ˆ76-100ï¼‰"),("front75","å‰75è¡Œï¼ˆ1-75ï¼‰"),("all100","å‰100è¡Œï¼ˆ1-100ï¼‰")],
                index=0, format_func=lambda t: t[1]
            )[0]
        else:
            range_key = "rear25"

    # â‘  åŠ è½½æ¨¡å‹ï¼ˆæ”¯æŒä¼šè¯æŒä¹…åŒ–ï¼‰
    st.subheader("â‘  åŠ è½½æ¨¡å‹")
    version_file = custom_ver.strip() if custom_ver.strip() else DEFAULT_VERSION_FILE[algo]
    if not os.path.isabs(version_file):
        version_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), version_file)

    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    cur_model, cur_classes, cur_features, cur_info, cur_ver = _get_model_from_state()
    if cur_model is not None:
        with st.status("å½“å‰æ¨¡å‹ï¼šå·²åŠ è½½ âœ…", expanded=False):
            st.write("version_file:", cur_ver)
            st.write("model_name:", cur_info.get("model_name", ""))
            st.write("features:", cur_features)
            if cur_classes: st.write("class_order:", cur_classes)

    colL, colR = st.columns([1,1])
    with colL:
        if st.button("åŠ è½½/åˆ·æ–°æ¨¡å‹", type="primary"):
            if not os.path.exists(version_file):
                st.error(f"ç‰ˆæœ¬æ–‡ä»¶ä¸å­˜åœ¨ï¼š{version_file}")
            else:
                with st.spinner("åŠ è½½æ¨¡å‹ä¸­..."):
                    model, classes, features, info = _really_load_model(version_file)
                    _store_model_to_state(model, classes, features, info, version_file)
                    st.success(f"æ¨¡å‹å·²åŠ è½½ï¼š{os.path.basename(version_file)}")
    with colR:
        if st.button("æ¸…é™¤å·²åŠ è½½æ¨¡å‹"):
            for k in ["mdl_obj","mdl_classes","mdl_features","mdl_info","mdl_version_file"]:
                st.session_state.pop(k, None)
            st.info("å·²æ¸…é™¤æ¨¡å‹ã€‚")

    # â‘¡ ä¸Šä¼  CSV å¹¶è¿è¡Œ
    st.subheader("â‘¡ ä¸Šä¼  CSVï¼ˆå¯å¤šé€‰ï¼‰")
    files = st.file_uploader("ä¸Šä¼  CSV æ–‡ä»¶ï¼ˆå¤šé€‰ï¼‰", type=["csv"], accept_multiple_files=True)

    if st.button("å¼€å§‹è¿è¡Œ", type="secondary"):
        model, classes_in_model, features_in_model, info_json, _ = _get_model_from_state()
        if model is None:
            st.error("æœªèƒ½åŠ è½½æ¨¡å‹ï¼Œè¯·å…ˆç‚¹å‡»â€œåŠ è½½/åˆ·æ–°æ¨¡å‹â€ã€‚"); st.stop()
        if not files:
            st.warning("è¯·å…ˆä¸Šä¼  CSV æ–‡ä»¶ã€‚"); st.stop()

        if mode == "eval":
            with st.spinner("æ„å»ºè¯„ä¼°é›†å¹¶æ¨ç†..."):
                X_test, y_test, classes_used = build_eval_set(files, drop_dup, sigma_mode, filename_rules)
                all_classes = classes_in_model if len(classes_in_model)>0 else sorted(list(set(classes_used)))
                st.write(f"æµ‹è¯•æ ·æœ¬æ•°ï¼š{len(y_test)}ï¼›ç±»åˆ«æ•°ï¼š{len(all_classes)}")
                if len(y_test) == 0:
                    st.warning("æ²¡æœ‰å¯è¯„ä¼°çš„æ ·æœ¬ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ æ–‡ä»¶ä¸æ–‡ä»¶åæ˜ å°„æ˜¯å¦æ­£ç¡®ã€‚")
                else:
                    eval_and_plot(model, X_test, y_test, all_classes)
        else:
            sigma_predict = "global" if sigma_mode == "global" else "none"
            if sigma_mode == "per_class":
                st.info("ä»…é¢„æµ‹æ¨¡å¼æ— æ³•æŒ‰ç±»åˆ«è¿‡æ»¤ï¼Œå·²è‡ªåŠ¨å¿½ç•¥è¯¥é€‰é¡¹ã€‚")
            with st.spinner("æ‰§è¡Œä»…é¢„æµ‹..."):
                predict_and_show(model, files, drop_dup, sigma_predict, range_key, classes_in_model)

    # â‘¢ æ‰‹åŠ¨è¾“å…¥å•æ ·æœ¬é¢„æµ‹ï¼ˆä½¿ç”¨ä¼šè¯ä¸­çš„æ¨¡å‹ï¼‰
    single_predict_ui()

    st.caption("æç¤ºï¼šæ¨¡å‹ä¸ç‰¹å¾é€‰æ‹©/æ ‡å‡†åŒ–å·²å°è£…åœ¨è®­ç»ƒå¯¼å‡ºçš„ pipeline ä¸­ï¼›æœ¬é¡µæ‰€æœ‰æ¨ç†å‡ä¸è®­ç»ƒä¿æŒä¸€è‡´ã€‚")

if __name__ == "__main__":
    main()
